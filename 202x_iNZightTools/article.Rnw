\documentclass{article}
\usepackage[utf8]{inputenc}

\usepackage{authblk}
\usepackage[authoryear,round]{natbib}
\usepackage{booktabs}
\usepackage{color}
\bibliographystyle{abbrvnat}

\usepackage{graphicx}
\usepackage{setspace}
\onehalfspacing

\usepackage{hyperref}

% commands
\newcommand{\proglang}[1]{\textsf{#1}}
\newcommand{\pkg}[1]{\textbf{#1}}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\fn}[1]{\code{#1()}}

\newcommand{\R}{\proglang{R}}
\newcommand{\inzight}{\pkg{iNZight}}


\usepackage[acronym]{glossaries}
\input{../glossary.tex}
\makeglossaries

\usepackage{cleveref}

%% For Sweave-based articles about R packages:
%% need no \usepackage{Sweave}
\SweaveOpts{engine=R, eps=FALSE, keep.source = TRUE}
<<setupdoc, echo=FALSE, results=hide>>=
source('../setup_doc.R')
authors <- paste(authors, collapse = ", ")
affils <- paste(affiliations, collapse = "\\\\\\\\ ")
@

\title{\Sexpr{title}}

\author{\Sexpr{authors}}
\affil{\Sexpr{affils}}
\date{}

\begin{document}

\maketitle

\begin{abstract}
\Sexpr{abstract}
\end{abstract}

\section{Introduction}
\label{sec:intro}

\inzight{} \citep{Elliott_2021} is a \gls{gui} for easy data exploration and visualisation. As part of its development, we needed wrapper packages to translate between the user interface and \R{} functions.
\begin{itemize}
    \item \gls{gui} has input fields user can population, equivalent to function arguments
    \item generally, one window talks to one function (but not always)
    \item often some complicated manipulation to go from user input values to valid function arguments
    \item doing this in a \gls{gui} is bad practice
    \item instead developed intermediary package that takes simple inputs and produces a \code{data.frame} output
    \item overtime this developed to also attach the \pkg{tidyverse} \citep{tidyverse} code used to perform the action
    \item simple function/argument interface makes it easier for beginners
\end{itemize}

There are a range of different classes of methods in \pkg{iNZightTools}. Many are data manipulation functions linking to \pkg{dplyr} \citep{dplyr} or similar. However there are others that are `clever' helper functions for choosing the correct function for a given situation.
\begin{itemize}
    \item import data from a range of file formats with \code{smart\_read()}
    \item import a survey design from a specification file format with \code{import\_survey()}
\end{itemize}


\section{Designing code-writing functions}\label{sec:code-writing-funs}

Unlike the majority of \R{} packages, \pkg{iNZightTools} provides little new functionality to users, but instead wraps existing functionality from a range of other packages in an attempt to provide a simple, stable \gls{api} for both users new to \R{}, but more importantly the \gls{gui} software package \inzight{}.
These \emph{wrapper functions} need to simplify the inputs to many more complex methods in such as way as to provide arguments that can be connected to graphical inputs such as dropdown boxes and sliders.
Second, they must compile and execute the necessary code to perform the required action(s).
A side-effect of this step is that the function generates code---in our case typically creating calls to \pkg{tidyverse} packages---which can be attached to the result and allow users to inspect the code required to perform the chosen action.
This produces a system by which users new to \R{} can begin exploring more complicated functions by modifying existing code.


\subsection{Choosing function arguments}\label{sec:arguments}

Arguably the most important part of designing a wrapper function is the choice of arguments.
By design, these methods are not supposed to provide the full set of functionality available from the underlying packages, but instead should provide a simple subset of features that are easy to access with minimal effort.
An important part of \inzight{} is the use of \emph{smart defaults}, and this starts off by specifying good defaults for as many arguments as possible.
Take, for example, a function for creating class intervals from a numeric variable.
The minimum information required would be the variable name, and everything else can provide ``good defaults'', for example the number of intervals.
In this way, \R{} users can call the function on a variable in a dataset and get a result instantly, and build up from there by specifying additional arguments.

Within \pkg{iNZightTools}, most of the argument choice has been decided by the requirements of the \gls{gui}, though in most cases this is a reasonable set of features for beginners to familiarize themselves with.
However, not all arguments are equal, and often some are dependent on others, while in other scenarios some arguments are needed only when another is ignored.
In these situations, we rely on \R{}'s lazy-evaluation to ignore unused arguments (which may depend on variables not defined).


\subsection{Constructing calls}\label{sec:constructing}

The basic framework \pkg{iNZightTools} uses to construct calls using \R{}'s \emph{expression} syntax, prefixing the call with \texttt{\~{}}.
The basic form of the call is then written out using placeholder variables in the parts that will be modified by the users.
In simple expressions, one single call is required, while in more complex methods several steps are often required, including context-specific steps (within \code{if-else} or \code{switch} statements).

Once the main structure of the call is complete, the individual arguments must be put together.
In some situations this is as simple as passing the argument from the function call, while in others it requires preparing data structures (such as a named vector or list), as required by the underlying function.
Details for some specific cases are given in the following sections.

\subsection{Evaluating calls and returning results}\label{sec:evaluate}

Once the components are ready, we pass the function through two methods: \code{replaceVars()} and \code{interpolate}.
The first, \code{replaceVars()}, substitutes placeholder variables with the names of created structures (named vectors and lists) which we wish to appear in the final code statement.
Secondly, \code{interpolate()} evaluates the expression in the current environment, but can accept additional arguments specifying the values of additional arguments (notably character values).
Not only does \code{interpolate()} evaluate the expression and return the result, but additionally attaches the expression to the objected, stored as a \code{"code"} attribute.
In all(?) cases, the result returned is a \code{data.frame} or \code{tibble}.

To extract the code from the returned object, there is the \code{iNZightTools::code()} function.
\glspl{gui} can use this to extract code from returned objects and stored it in code history, while users can examine the code used and modify it to access more advanced aspects of the underlying methods.

In the following sections, I will describe some specific implementations for three important components of the \pkg{iNZightTools} package: \emph{data import}, \emph{data wrangling}, and \emph{variable manipulation}.



\section{Importing Data}\label{sec:import-data}

\begin{itemize}
    \item import data with the \code{smart\_read()} function
    \item uses file extension to guess best package and function to use, e.g., \code{.xlsx} uses \code{haven::read\_excel()} \citep{haven}
    \item also handles metadata parsing for \gls{csv}
    \item subsection about datatypes (numeric, factor/categorical, and date-time), with links to subsections with details
\end{itemize}


<<filetypes,echo=FALSE,results=tex>>=
types_df <- tibble::tribble(
    ~"Extension", ~"Format", ~"Function(s)", ~"Package",
    "txt", "Tab Delimited File", "read_text/read_delim", "readr",
    "csv", "Comma Separated Values File", "read_csv/read_delim", "readr",
    "sav", "SPSS Save File", "read_sav", "haven",
    "sas7bdat, xpt", "SAS Data Files, SAS XPORT Files", "read_sas, read_xpt", "haven",
    "xls, xlsx", "Excel Files", "read_excel", "readxl",
    "dta", "STATA Files", "read_dta", "haven",
    "json", "JSON Data", "fromJSON", "jsonlite",
    "rds", "Serialized R Object", "readRDS", "base",
    "svydesign", "Survey Design File", "import_survey", "surveyspec"
)
Ntypes <- types_df$Extension |>
    stringr::str_split(", ") |> unlist() |> length()
knitr::kable(types_df, "latex",
    caption = "File types supported by smart\\_read().",
    label = "file-types",
    booktabs = TRUE,
    linesep = ""
) |> kableExtra::kable_styling(font_size = 7)
@

Before any kind of analysis can be undertaken, users mustfirst import their dataset into \R{}.
This can be an arduos process for novices, especially if the data is not in a \gls{csv} format and thus requires use of an additional \R{} package to import.
Similarly, trying to design an \R{} \gls{gui} module to import data in a wide range of formats would become quite complicated as the number of formats increase.
Our solution was to create a single function, \fn{smart\_read}, that takes a file path or URL as the only required argument and imports the data using the file extension (.csv, .xlsx, etc) to figure out the appropriate package and function to use to import it.
At the time of writing, \fn{smart\_read} can read \Sexpr{Ntypes} file extensions, listed in \cref{tab:file-types}.


When combined with the code writing functionality from section 2, this provides a powerful way for beginners and non-beginners alike to quickly import a dataset without first figuring out what function to use.
Once the initial read has been performed, it can be examined and if changes are needed, these can either be made to the call to \fn{smart\_read}, or by checking the code and modifying it as needed.

\subsection{Parsing Metadata for Easier Data Distribution and Import}\label{sec:metadata}

\begin{itemize}
    \item often data coded (factors as numbers instead of labels)
    \item users need to refer to information (often external) to first set-up the variables correctly before they can get started with visualisation
    \item this is hard/not feasible for novice users
    \item metadata can be included in/distributed with the raw data
    \item \code{smart\_read()} will parse the metadata and apply transformations
    \item here are some examples
\end{itemize}

On top of the standard data import possibilities, \pkg{iNZightTools} also includes its own ``metadata'' specification, allowing data distributers to specify information about the variables in the dataset.
This format will be familiar to users acquainted with \pkg{roxygen2} \cite{}.
Some key features include automatically coding integer values as factors, which can greatly reduce the size of a \gls{csv} file but skip a painful step (particularly for \gls{gui} users).

The \fn{smart\_read} function automatically checks for metadata at the top of a \gls{csv} file, and parses it if present.
This triggers modification of function arguments, for example \fn{readr::read\_csv}, where possible (for example when parsing integers as a factor variable), and in other cases uses \fn{dplyr::mutate} to modify the data object returned, for example if factor levels are renamed or combined.

\subsection{Data types}\label{sec:data-types}

There are many data types understood by \R{}, however for most data analysis situations these can be condensed into numeric, factors/categorical, and date-times.
\pkg{iNZightTools} automatically coerces all variables to one of these (characters to factors for historic reasons, and restricts the values a variable can take, particularly useful for novice users interfacing through a \gls{gui}).
Of course, the code used to do this is all generated using the principles of \cref{sec:code-writing-funs}, so can be viewed by the user and modified as needed.

Date-time variables are read automatically if the package supports it (e.g., \fn{readr::read\_csv}).
The format depends on whether the value is a date, a datetime, or a time (``duration'').
\Cref{tab:dt-types} shows which value types are used, and the package used to support this.
Working with these types of variables is facilitated by some variable manipulation methods, which are described in \cref{sec:variable-manipulation}.

<<filetypes,echo=FALSE,results=tex>>=
types_df <- tibble::tribble(
    ~"Type", ~"Examples", ~"Constructor function", ~"Package",
    "Date", "2021-08-14", "as.Date()", "base",
    "Datetime", "2021-08-14 13:04:35", "as.POSIXct()", "base",
    "Time/Duration", "13:04:35", "as_hms()", "hms"
)
knitr::kable(types_df, "latex",
    caption = "Date-time formats supported.",
    label = "dt-types",
    booktabs = TRUE,
    linesep = ""
) |> kableExtra::kable_styling(font_size = 7)
@


\section[Data wrangling with iNZightTools]{Data wrangling with \pkg{iNZightTools}}\label{sec:data-wrangling}

\begin{itemize}
    \item a bunch of methods: filter, sort, aggregate, join
    \item here are some examples
    \item and accessing the code
    \item dataset validation
\end{itemize}



\section{Variable manipulation methods}\label{sec:variable-manipulation}

\begin{itemize}
    \item renaming, releveling, reordering, transforming
    \item some date-time specific operations
\end{itemize}


\section{Conclusion}\label{sec:conclusion}

\begin{itemize}
    \item summarize
    \item what's next (plots, GUI, etc? maybe?)
\end{itemize}


\section*{Acknowledgements}
\inzight{} is free software developed by students of the Department of Statistics, University of Auckland. Recent work has been made possible by Te Rourou TƒÅtaritanga (\url{https://terourou.org)}, a research group at Victoria University of Wellington funded by an MBIE Endeavour Grant, ref 62506 ENDRP, with additional funding from iNZight Analytics.


\printglossary[type=\acronymtype]

\bibliography{../references.bib}


\end{document}
